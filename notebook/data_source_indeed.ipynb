{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install cloudscraper\n",
    "# !pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from urllib.parse import quote_plus, urlparse, ParseResult\n",
    "import math\n",
    "import time\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers():\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\"\n",
    "    ]\n",
    "\n",
    "    headers = {\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "    }\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs(job_search):\n",
    "    \"\"\"\n",
    "    This function scrapes job postings from Simply Hired for a given job search term. \n",
    "    It paginates through all the available job listings, and for each job, it extracts \n",
    "    the job title, location, company name, link to the job post, and a summary of the job.\n",
    "    \n",
    "    The function makes use of the 'cloudscraper' and 'BeautifulSoup' libraries for \n",
    "    making HTTP requests and parsing the HTML response respectively.\n",
    "    \n",
    "    To avoid getting blocked, the function includes a delay of 3 seconds between \n",
    "    requests when paginating through job listings.\n",
    "    \n",
    "    The extracted job data is returned as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    job_search (str): The job search term. Spaces and special characters in the term \n",
    "                      are URL-encoded.\n",
    "\n",
    "    Returns:\n",
    "    df (pandas.DataFrame): A DataFrame containing the scraped job data. Each row \n",
    "                           corresponds to a job post, and the columns are 'title',\n",
    "                           'job_location', 'company_name', 'job_link', and 'job_summary'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode job_search for URL (replace spaces with '+' and other special characters)\n",
    "    job_search = quote_plus(job_search)  \n",
    "\n",
    "    # Base URL for Simply Hired\n",
    "    base_url = \"https://www.simplyhired.ca\"\n",
    "\n",
    "    # Construct URL with search query\n",
    "    url = f\"{base_url}/search?q={job_search}\"\n",
    "\n",
    "    # Create a scraper instance\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "\n",
    "    # Get headers\n",
    "    headers = get_headers()\n",
    "\n",
    "    # Make an HTTP GET request to the URL\n",
    "    response = scraper.get(url, headers=headers)\n",
    "\n",
    "    # Parse the HTML content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find total job count and calculate the number of pages to scrape\n",
    "    total_job_count = int(soup.find('span', {'class': 'posting-total'}).text)\n",
    "    job_pages = math.ceil(total_job_count/20)\n",
    "    \n",
    "    # List to store data for all jobs\n",
    "    jobs_data = []\n",
    "\n",
    "    # Loop through all pages\n",
    "    for page in range(1, job_pages + 1):\n",
    "        # Print the page number\n",
    "        print(f\"Scraping page {page}...\")\n",
    "\n",
    "        # If it's not the first page, make another request and parse it\n",
    "        if page > 1:  \n",
    "            # Sleep for 3 seconds between requests to avoid being blocked\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "            \n",
    "            url = f\"{base_url}/search?q={job_search}&pn={page}\"\n",
    "            response = scraper.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the list of jobs on the page\n",
    "        job_list = soup.find('ul', {'class': 'jobs'})\n",
    "        jobs  = job_list.findAll('div', {'class': 'SerpJob-jobCard'})\n",
    "\n",
    "        # Loop through all jobs on the page\n",
    "        for job in jobs:\n",
    "            # Extract job details and construct job URL\n",
    "            title_tag = job.find('h3', {'class': 'jobposting-title'})\n",
    "            title = title_tag.text\n",
    "            link = title_tag.find('a').attrs['data-mdref']\n",
    "            \n",
    "            # Remove query parameters from the job URL\n",
    "            parsed_link = urlparse(link)\n",
    "            link = ParseResult(scheme=parsed_link.scheme, netloc=parsed_link.netloc, \n",
    "                               path=parsed_link.path, params=parsed_link.params, \n",
    "                               query='', fragment=parsed_link.fragment).geturl()\n",
    "\n",
    "            # Extract other job details\n",
    "            company_name = job.find('span', {'class': 'jobposting-company'}).text\n",
    "            job_location = job.find('span', {'class': 'jobposting-location'}).text\n",
    "            job_summary = job.find('p', {'class': 'jobposting-snippet'}).text\n",
    "            date_of_job_post = job.find('time').attrs['datetime']\n",
    "            \n",
    "            url = f\"https://www.simplyhired.ca{link}?isp=0&q={job_search}\"\n",
    "\n",
    "            # Store job details in a dictionary and append it to our list\n",
    "            data = {\n",
    "                'date_of_job_post': date_of_job_post,\n",
    "                'title':  title,\n",
    "                'job_location': job_location,\n",
    "                'company_name': company_name,\n",
    "                'job_link': url,\n",
    "                'job_summary': job_summary\n",
    "            }\n",
    "\n",
    "\n",
    "            # jobs_data.append(data)\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.drop_duplicates(subset='job_link', keep=\"first\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 507 entries, 0 to 611\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   date_of_job_post  507 non-null    object\n",
      " 1   title             507 non-null    object\n",
      " 2   job_location      507 non-null    object\n",
      " 3   company_name      507 non-null    object\n",
      " 4   job_link          507 non-null    object\n",
      " 5   job_summary       507 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 27.7+ KB\n"
     ]
    }
   ],
   "source": [
    "jobs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.to_csv('data/data_scientist_jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.simplyhired.ca/job/Tiz9efu8Gbf2yqVK4oCuf-uH2etT7S4517IS2UMqTonAM9lHWqIABQ?isp=0&q=Data+Scientist\n",
       "1    https://www.simplyhired.ca/job/gnMlpa9NCMhcAHjRRpuwL4pMsTR_w9p4V7KqY91sXoL_QspI1c_psQ?isp=0&q=Data+Scientist\n",
       "2    https://www.simplyhired.ca/job/dnDqxJfbA0BNLi3s7qXBBqvflRIknt3mZHDE05ilVEpYat7yGyHm0A?isp=0&q=Data+Scientist\n",
       "3    https://www.simplyhired.ca/job/VAesZ_CXrODXtd7s3vMg6A8kV2wZZBuurPvnoAwGhRjbmZ1ZjhuYTg?isp=0&q=Data+Scientist\n",
       "4    https://www.simplyhired.ca/job/hz4vgNdlPpqaBUZjfQNZ041OYmCoR6OlY19d_nh0E81eMPJcj8OCUg?isp=0&q=Data+Scientist\n",
       "5    https://www.simplyhired.ca/job/NfZT4eMIwgIGaN76LkgqnkgsIQEsykQo2txVKRw6tHWUM_5mlZ32Zw?isp=0&q=Data+Scientist\n",
       "6    https://www.simplyhired.ca/job/eDV2-7kb1ayYcwuRv5D7ODlcDt0sHR8PkxdCyTajEOMEnbnOE6CuAg?isp=0&q=Data+Scientist\n",
       "7    https://www.simplyhired.ca/job/NkJjM_dzhGkWQLkM7IdXV0aWj00h65IyiG-to5x-YfMqQPol04oo-g?isp=0&q=Data+Scientist\n",
       "8    https://www.simplyhired.ca/job/y1lefm-wNMoxBUDzkGAPikKUwYc_gVVbI6oGzOEd7rckT7jpTfsC1A?isp=0&q=Data+Scientist\n",
       "9    https://www.simplyhired.ca/job/P0SqCYiFkTM3HHp8NrJ8QIcgc4tztzNVWyuoWn3WyjjEcQraC-GCaw?isp=0&q=Data+Scientist\n",
       "Name: job_link, dtype: object"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "jobs_df['job_link'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_details(job_url):\n",
    "    \"\"\"\n",
    "    This function takes in a URL of a job posting, fetches the page,\n",
    "    and extracts additional details like job type, qualifications, and description.\n",
    "    \"\"\"\n",
    "    # Create a new scraper for each job detail page\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    headers = get_headers()\n",
    "    response = scraper.get(job_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract additional info, like job description, job type, qualifications, etc.\n",
    "    job_details = soup.find('div', {'class': 'viewjob-content'})\n",
    "\n",
    "    # Job type\n",
    "    job_type_tag = job_details.find('span', {'class': 'viewjob-jobType'})\n",
    "    job_type = job_type_tag.text if job_type_tag else None  # Default to \"N/A\" if tag is not found\n",
    "\n",
    "    # Job qualifications\n",
    "    qualifications_tags = job_details.findAll('li', {'class': 'viewjob-qualification'})\n",
    "    job_qualifications = [qual_tag.text for qual_tag in qualifications_tags] if qualifications_tags else None\n",
    "\n",
    "    # Job description\n",
    "    job_description_tag = job_details.find('div', {'data-testid': 'VJ-section-content-jobDescription'})\n",
    "    job_description = job_description_tag.text if job_description_tag else None\n",
    "\n",
    "    return {\n",
    "        'job_type': job_type,\n",
    "        'job_qualifications': job_qualifications,\n",
    "        'job_description': job_description\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    job_data_generator = scrape_jobs(\"Data Scientist\")\n",
    "    jobs_df = pd.DataFrame(job_data_generator)\n",
    "\n",
    "    #  Use ThreadPoolExecutor to parallelize the fetching of job details\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Fetch job details for each job link\n",
    "        job_details_list = list(executor.map(get_job_details, jobs_df['job_link'].tolist()))\n",
    "\n",
    "    # Create a DataFrame from the list of job details\n",
    "    jobs_details_df = pd.DataFrame(job_details_list)\n",
    "\n",
    "    # Combine the original DataFrame with the job details DataFrame\n",
    "    jobs_df = pd.concat([jobs_df, jobs_details_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.drop_duplicates(subset='job_link', keep=\"first\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 508 entries, 0 to 596\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   date_of_job_post    508 non-null    object\n",
      " 1   title               508 non-null    object\n",
      " 2   job_location        508 non-null    object\n",
      " 3   company_name        508 non-null    object\n",
      " 4   job_link            508 non-null    object\n",
      " 5   job_summary         508 non-null    object\n",
      " 6   job_type            377 non-null    object\n",
      " 7   job_qualifications  508 non-null    object\n",
      " 8   job_description     508 non-null    object\n",
      "dtypes: object(9)\n",
      "memory usage: 39.7+ KB\n"
     ]
    }
   ],
   "source": [
    "jobs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the enriched DataFrame as a CSV file\n",
    "jobs_df.to_csv('data/data_scientist_jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Indeed jobs\n",
    "# count = window.mosaic.providerData['mosaic-provider-jobcards'].metaData.mosaicProviderJobCardsModel.tierSummaries[0].jobCount\n",
    "# window.mosaic.providerData['mosaic-provider-jobcards'].metaData.mosaicProviderJobCardsModel.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from urllib.parse import quote_plus, urlparse, ParseResult\n",
    "import math\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class SimplyHiredJobScraper:\n",
    "    def __init__(self, job_roles, locations):\n",
    "        self.job_roles = job_roles\n",
    "        self.locations = locations\n",
    "        self.base_url = \"https://www.simplyhired.ca\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_headers():\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\"\n",
    "        ]\n",
    "\n",
    "        headers = {\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": random.choice(user_agents),\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "        }\n",
    "\n",
    "        return headers\n",
    "    \n",
    "\n",
    "    def scrape_jobs(self, job_role, location):\n",
    "        # Encode job_search for URL (replace spaces with '+' and other special characters)\n",
    "        job_role = quote_plus(job_role)\n",
    "\n",
    "        # Encode location\n",
    "        location = quote_plus(location)\n",
    "\n",
    "        # Construct URL with search query\n",
    "        url = f\"{self.base_url}/search?q={job_role}&l={location}\"\n",
    "\n",
    "        # Create a scraper instance\n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        \n",
    "\n",
    "\n",
    "    def get_job_details(self, job_url):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
